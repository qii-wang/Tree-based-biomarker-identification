---
title: "Comparison of different tree-based methods - RF Imputation"
author: "Qi Wang"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readxl)
library(missForest)
library(ggplot2)
set.seed(1024)
full_dat = as.data.frame(read_xlsx("2024-03-25_datasplit.xlsx"))
full_dat_to_be_imputed = full_dat[, -c(10,11,12)]
full_dat_to_be_imputed$productInfusedBinaryBrexucel = as.factor(full_dat_to_be_imputed$productInfusedBinaryBrexucel)
full_dat_imputed = cbind(missForest(full_dat_to_be_imputed)$ximp,
                         icans_grade_3plus = full_dat$icans_grade_3plus)

train_dat = full_dat_imputed[which(full_dat$TestSet == 0),]
test_dat = full_dat_imputed[which(full_dat$TestSet == 1),]
```

# XGBoost
```{r}
library(xgboost)
library(pROC)
library(epiR)
set.seed(1024)
n_boot = 1000

x_train = train_dat[, -ncol(train_dat)]
# x_train$productInfusedBinaryBrexucel = as.factor(x_train$productInfusedBinaryBrexucel)
y_train = train_dat$icans_grade_3plus
x_train_df = model.matrix(~.-1, data = x_train)
xgb_df = xgb.DMatrix(data = x_train_df, label = y_train)

xgboost_para_tune = expand.grid(max_depth = c(3,5,7),
                                test_auc = 0,
                                round = 0)
for(iter in 1:nrow(xgboost_para_tune)){
  params = list(max_depth = xgboost_para_tune$max_depth[iter])
  bst = xgb.cv(params = params,,
               data = xgb_df, nrounds = 500,
               objective = "binary:logistic", early_stopping_rounds = 10,
               metrics = "auc",
               nfold = 5,
               maximize = TRUE,
               verbose = FALSE)
  xgboost_para_tune$test_auc[iter] = bst$evaluation_log$test_auc_mean[bst$best_iteration]
  xgboost_para_tune$round[iter] = bst$best_iteration
}

xgboost_para_tune[which.max(xgboost_para_tune$test_auc),]

bst_optimal = xgboost(data = xgb_df,
                      nrounds = xgboost_para_tune$round[which.max(xgboost_para_tune$test_auc)],
                      max.depth = xgboost_para_tune$max_depth[which.max(xgboost_para_tune$test_auc)],
                      objective = "binary:logistic")

y_test_pred_xgb = predict(bst_optimal, newdata = model.matrix(~.-1, test_dat[,-ncol(test_dat)]))
roc_xgb = roc(test_dat$icans_grade_3plus, y_test_pred_xgb)
roc_xgb$auc
ci.auc(roc_xgb, method = "bootstrap")


y_train_pred_xgb = predict(bst_optimal, newdata = x_train_df)
roc_train_xgb = roc(train_dat$icans_grade_3plus, y_train_pred_xgb)
roc_train_xgb$auc
ci.auc(roc_train_xgb, method = "bootstrap")

opt_thre_xgb = roc_train_xgb$thresholds[which.max(roc_train_xgb$sensitivities + roc_train_xgb$specificities)]
y_test_pred_label_xgb = as.integer(y_test_pred_xgb > opt_thre_xgb)
epi.tests(table(y_test_pred_label_xgb, test_dat$icans_grade_3plus)[2:1,2:1], digits = 3)

y_train_pred_label_xgb = as.integer(y_train_pred_xgb > opt_thre_xgb)
epi.tests(table(y_train_pred_label_xgb, train_dat$icans_grade_3plus)[2:1,2:1], digits = 3)


mean(y_train_pred_label_xgb == y_train)

mean(y_test_pred_label_xgb == test_dat$icans_grade_3plus)


xgb_imp_df = xgb.importance(model = bst_optimal)
imp_plot_xgb = ggplot(aes(x = Feature, y = Gain), data = xgb_imp_df) +
  geom_bar(stat="identity")+
  coord_flip()+
  theme_minimal()+
  ylab("XGBoost_Importance")
```

# BART
```{r}
library(bartMachine)
set.seed(1024)
N = nrow(train_dat)
num_folds = 5
folds = cut(seq(1, N), breaks=num_folds, labels=FALSE)
folds = folds[sample(1:N, size = N, replace = FALSE)]
bart_para_tune = expand.grid(num_trees = c(20, 50, 100, 150),
                             test_auc = 0)
for(i in 1:nrow(bart_para_tune)){
  cat(i,"\n")
  num_trees = bart_para_tune$num_trees[i]
  cv_auc = NULL
  for(j in 1:num_folds){
    cv_test_id = which(folds == j)
    cv_x_train = x_train[-cv_test_id, ]
    cv_y_train = y_train[-cv_test_id]
    cv_x_test = x_train[cv_test_id, ]
    cv_y_test = y_train[cv_test_id]
    cv_bart = bartMachine(X = cv_x_train, y = as.factor(cv_y_train),
                          num_trees = num_trees,
                          verbose = FALSE,
                          num_burn_in = 500,
                          num_iterations_after_burn_in = 2000)
    cv_y_pred = predict(cv_bart, new_data = cv_x_test)
    cv_auc[j] = auc(cv_y_test, cv_y_pred)
  }
  bart_para_tune$test_auc[i] = mean(cv_auc)
}

bart_para_tune[which.max(bart_para_tune$test_auc),]

bart_optimal = bartMachine(X = x_train, y = as.factor(y_train),
                          num_trees = bart_para_tune$num_trees[which.max(bart_para_tune$test_auc)],
                          num_burn_in = 500,
                          num_iterations_after_burn_in = 2000)
y_test_pred_bart = predict(bart_optimal, new_data = test_dat[, -ncol(test_dat)])
roc_bart = roc(test_dat$icans_grade_3plus, y_test_pred_bart)
roc_bart$auc
ci.auc(roc_bart, method = "bootstrap")

y_train_pred_bart = predict(bart_optimal, new_data = x_train)
roc_train_bart = roc(train_dat$icans_grade_3plus, y_train_pred_bart)
roc_train_bart$auc
ci.auc(roc_train_bart, method = "bootstrap")

opt_thre_bart = roc_train_bart$thresholds[which.max(roc_train_bart$sensitivities + roc_train_bart$specificities)]
y_test_pred_label_bart = as.integer(y_test_pred_bart > opt_thre_bart)
epi.tests(table(y_test_pred_label_bart, test_dat$icans_grade_3plus)[2:1,2:1], digits = 3)

y_train_pred_label_bart = as.integer(y_train_pred_bart > opt_thre_bart)
epi.tests(table(y_train_pred_label_bart, train_dat$icans_grade_3plus)[2:1,2:1], digits = 3)

mean(y_train_pred_label_bart == y_train)
mean(y_test_pred_label_bart == test_dat$icans_grade_3plus)

bart_var_sel = var_selection_by_permute(bart_optimal, num_reps_for_avg = 10,
                         num_permute_samples = 5,
                         num_trees_for_permute = bart_para_tune$num_trees[which.max(bart_para_tune$test_auc)], 
                         alpha = 0.05, plot = FALSE)

imp_plot_bart = ggplot(aes(x = Feature, y = BART_Importance),
                       data = data.frame(Feature = names(bart_var_sel$var_true_props_avg),
                                         BART_Importance = bart_var_sel$var_true_props_avg)) +
  geom_bar(stat="identity")+
  coord_flip()+
  theme_minimal()
```


# Random forest
```{r}
library(randomForestSRC)
set.seed(1024)

rf_para_tune = expand.grid(mtry = c(2,3,4,5),
                           nodesize = c(1,3,5),
                           test_auc = 0,
                           stringsAsFactors = FALSE)
for(i in 1:nrow(rf_para_tune)){
  cat(i, "\n")
  mtry = rf_para_tune$mtry[i]
  nodesize = rf_para_tune$nodesize[i]
  cv_auc = NULL
  for(j in 1:num_folds){
    cv_test_id = which(folds == j)
    cv_x_train = x_train[-cv_test_id, ]
    cv_y_train = y_train[-cv_test_id]
    cv_x_test = x_train[cv_test_id, ]
    cv_y_test = y_train[cv_test_id]
    cv_rf = rfsrc(icans_grade_3plus ~.,
                  data = cbind(cv_x_train, icans_grade_3plus = as.factor(cv_y_train)),
                  mtry = mtry, nodesize = nodesize,)
    cv_y_pred = predict(cv_rf, newdata = cv_x_test)$predicted[, "1"]
    cv_auc[j] = auc(cv_y_test, cv_y_pred)
  }
  rf_para_tune$test_auc[i] = mean(cv_auc)
}

best_para_id = which.max(rf_para_tune$test_auc)
rf_para_tune[best_para_id, ]
rf_optimal = rfsrc(icans_grade_3plus ~.,
                   data = cbind(x_train, icans_grade_3plus = as.factor(y_train)),
                   mtry = rf_para_tune$mtry[best_para_id],
                   nodesize = rf_para_tune$nodesize[best_para_id])
y_test_pred_rf = predict(rf_optimal, newdata = test_dat[, -ncol(test_dat)])$predicted[, "1"]
roc_rf = roc(test_dat$icans_grade_3plus, y_test_pred_rf)
roc_rf$auc
ci.auc(roc_rf, method = "bootstrap")

y_train_pred_rf = predict(rf_optimal, newdata = x_train)$predicted[, "1"]
roc_train_rf = roc(train_dat$icans_grade_3plus, y_train_pred_rf)
roc_train_rf$auc
ci.auc(roc_train_rf, method = "bootstrap")

opt_thre_rf = roc_train_rf$thresholds[which.max(roc_train_rf$sensitivities + roc_train_rf$specificities)]
y_test_pred_label_rf = as.integer(y_test_pred_rf > opt_thre_rf)
epi.tests(table(y_test_pred_label_rf, test_dat$icans_grade_3plus)[2:1,2:1], digits = 3)

y_train_pred_label_rf = as.integer(y_train_pred_rf > opt_thre_rf)
epi.tests(table(y_train_pred_label_rf, train_dat$icans_grade_3plus)[2:1,2:1], digits = 3)

mean(y_train_pred_label_rf == y_train)
mean(y_test_pred_label_rf == test_dat$icans_grade_3plus)

imp_rf = vimp(rf_optimal, importance = "permute")

imp_plot_rf = ggplot(aes(x = Feature, y = RF_Importance),
                       data = data.frame(Feature = rownames(imp_rf$importance), RF_Importance = imp_rf$importance[, "all"])) +
  geom_bar(stat="identity")+
  coord_flip()+
  theme_minimal()
```

# Conditional random forest
```{r}
library(party)
set.seed(1024)
crf_para_tune = expand.grid(mtry = c(2,3,4,5),
                            test_auc = 0)

for(i in 1:nrow(crf_para_tune)){
  cat(i, "\n")
  mtry = crf_para_tune$mtry[i]
  cv_auc = NULL
  for(j in 1:num_folds){
    cv_test_id = which(folds == j)
    cv_x_train = x_train[-cv_test_id, ]
    cv_y_train = y_train[-cv_test_id]
    cv_x_test = x_train[cv_test_id, ]
    cv_y_test = y_train[cv_test_id]
    cv_crf = cforest(icans_grade_3plus ~.,
                     data = cbind(cv_x_train, icans_grade_3plus = as.factor(cv_y_train)),
                     controls = cforest_unbiased(mtry = mtry))
    cv_y_pred = sapply(predict(cv_crf, newdata = cv_x_test, type = "prob"), function(x){x[2]})
    cv_auc[j] = auc(cv_y_test, cv_y_pred)
  }
  crf_para_tune$test_auc[i] = mean(cv_auc)
}

crf_best_para_id = which.max(crf_para_tune$test_auc)
crf_para_tune[crf_best_para_id,]

crf_optimal = cforest(icans_grade_3plus ~.,
                   data = cbind(x_train, icans_grade_3plus = as.factor(y_train)),
                   controls = cforest_unbiased(mtry = crf_para_tune$mtry[crf_best_para_id]))
y_test_pred_crf = sapply(predict(crf_optimal, newdata = test_dat[, -ncol(test_dat)],
                                 type = "prob"), function(x){x[2]})
roc_crf = roc(test_dat$icans_grade_3plus, y_test_pred_crf)
roc_crf$auc
ci.auc(roc_crf, method = "bootstrap")

y_train_pred_crf = sapply(predict(crf_optimal, newdata = x_train,
                                 type = "prob"), function(x){x[2]})
roc_train_crf = roc(train_dat$icans_grade_3plus, y_train_pred_crf)
roc_train_crf$auc
ci.auc(roc_train_crf, method = "bootstrap")

opt_thre_crf = roc_train_crf$thresholds[which.max(roc_train_crf$sensitivities + roc_train_crf$specificities)]
y_test_pred_label_crf = as.integer(y_test_pred_crf > opt_thre_crf)
epi.tests(table(y_test_pred_label_crf, test_dat$icans_grade_3plus)[2:1,2:1], digits = 3)

y_train_pred_label_crf = as.integer(y_train_pred_crf > opt_thre_crf)
epi.tests(table(y_train_pred_label_crf, train_dat$icans_grade_3plus)[2:1,2:1], digits = 3)

mean(y_train_pred_label_crf == y_train)
mean(y_test_pred_label_crf == test_dat$icans_grade_3plus)

crf_imp = varimp(crf_optimal, pre1.0_0 = TRUE)
imp_plot_crf = ggplot(aes(x = Feature, y = CRF_Importance),
                       data = data.frame(Feature = names(crf_imp),
                                         CRF_Importance = crf_imp)) +
  geom_bar(stat="identity")+
  coord_flip()+
  theme_minimal()
```

# Analyze results
```{r}
plot(smooth(roc_xgb), col = "blue")
plot(smooth(roc_bart), col = "red", add = TRUE)
plot(smooth(roc_rf), col = "green", add = TRUE)
plot(smooth(roc_crf), col = "purple", add = TRUE)
legend("bottomright", legend=c("XGBoost", "BART", "RF", "CondRF"),
       col=c("blue", "red", "green", "purple"), lwd=2)


plot(roc_train_xgb, col = "blue")
plot(roc_train_bart, col = "red", add = TRUE)
plot(roc_train_rf, col = "green", add = TRUE)
plot(roc_train_crf, col = "purple", add = TRUE)
legend("bottomright", legend=c("XGBoost", "BART", "RF", "CondRF"),
       col=c("blue", "red", "green", "purple"), lwd=2)

library(gridExtra)
imp_plot = grid.arrange(imp_plot_xgb, imp_plot_bart, imp_plot_rf, imp_plot_crf, nrow = 2, ncol = 2)
ggsave("Importance plot.png", imp_plot, width = 12, height = 6)
```